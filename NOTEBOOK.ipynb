{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import lyricsgenius\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import duden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Lyrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- API Schlüssel und Zugangsdaten einsetzen\n",
    "- Playlist IDs von Spotify suchen \n",
    "- Jeder Track der Playlist wird auf LyricsGenius gesucht\n",
    "- Sollte zu einem Song kein Lyrics gefunden werden, wird die Anzahl hochgezählt\n",
    "- Es wird eine JSON Datei gebildet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPOTIPY_CLIENT_ID = '46da6046625f409cb53ab06bf807861d'\n",
    "SPOTIPY_CLIENT_SECRET = '00efb0ab68db4f1eaf27ec53a4c1e068'\n",
    "GENIUS_ACCESS_TOKEN = 'OsBvEsJ_OLKYR42CJoaA_N97GNOS44XXFk4KPPcLtCYnWmDwnnzY2RpcG_SdG25a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_id = '2siI2ILJEPr88yibWLgQQl'  #Playlist: \"Deutsche Songs die jeder kennt ;)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführen von getlyrics.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./all_songs.json\") as f:\n",
    "    songs_dict = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführen von cyspa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:20<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#NEU #Spacy muss gedownloaded werden mit : pip install -U pip setuptools wheel   und    pip install spacy \n",
    "#de_core_news_sm-3.5.0-py3-none-any.whl (muss gedownloaded werden -> https://spacy.io/models/de/)\n",
    "#de_core_news_lg-3.5.0-py3-none-any.whl\n",
    "#de_dep_news_trf-3.5.0-py3-none-any.whl\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "#nlp1 = spacy.load (\"de_dep_news_trf\")\n",
    "nlp2 = spacy.load (\"de_core_news_lg\")\n",
    "\n",
    "list_of_nouns, list_of_pronouns = [], []\n",
    "\n",
    "def spacify_with_coref(lyrics_from_song):\n",
    "    doc3 = nlp2(lyrics_from_song)\n",
    "    doc = nlp(lyrics_from_song)\n",
    "\n",
    "    doc._.coref_chains = doc3._.coref_chains\n",
    "\n",
    "    return doc\n",
    "\n",
    "for song_id in tqdm(range(len(songs_dict[\"Prototyp\"]))):\n",
    "\n",
    "    lyrics_from_song = songs_dict[\"Prototyp\"][str(song_id)][\"lyrics\"]\n",
    "    doc = nlp(lyrics_from_song)\n",
    "    #doc2 = nlp1 (lyrics_from_song)\n",
    "    doc3 = nlp2 (lyrics_from_song)\n",
    "\n",
    "    # Ausgabe lediglich von Wörtern, die Nomen und Pronomen sind:\n",
    "    for token in doc3: \n",
    "        if token.pos_ == \"NOUN\":\n",
    "            list_of_nouns.append(token.text)\n",
    "        elif token.pos_ == \"PRON\":\n",
    "            list_of_pronouns.append(token.text)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6516 nouns were found.\n",
      "5504 pronouns were found.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(list_of_nouns)} nouns were found.\")\n",
    "print(f\"{len(list_of_pronouns)} pronouns were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2169 distinct nouns were found.\n",
      "110 distinct pronouns were found.\n"
     ]
    }
   ],
   "source": [
    "distinct_nouns = sorted(list(set(list_of_nouns)))\n",
    "distinct_pronouns = sorted(list(set(list_of_pronouns)))\n",
    "print(f\"{len(distinct_nouns)} distinct nouns were found.\")\n",
    "print(f\"{len(distinct_pronouns)} distinct pronouns were found.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Daten sortieren\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOMEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json-Datei wird erstellt, um eine Liste mit allen Nomen und zugehörigen Artikeln zu bekommen. (Durchlaufzeit 28 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary\n",
    "result_nomen = {\"Nomen\": {}}\n",
    "\n",
    "unknown_nomen = 0\n",
    "id = 0\n",
    "for word in list_of_nouns:\n",
    "        n = duden.get(word)  \n",
    "        if n is not None: \n",
    "                result_nomen['Nomen'][id] = {\"WORT\": n.name, \"ARTIKEL\": n.article}\n",
    "                id += 1\n",
    "        else:\n",
    "                unknown_nomen +=1\n",
    "    \n",
    "    #pb.update(1)\n",
    "with open('Nomen.json', 'w') as fp:\n",
    "      dict_result_nomen=json.dump(result_nomen, fp, indent=4)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einzelne Json-Dateien für Nomen (m,w,n) erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomen_m , nomen_w, nomen_n = [],[],[]    \n",
    "for id in result_nomen[\"Nomen\"]: \n",
    "    if  result_nomen [\"Nomen\"][id][\"ARTIKEL\"]== \"der\":\n",
    "        nomen_m.append(result_nomen[\"Nomen\"][id][\"WORT\"])\n",
    "    elif result_nomen [\"Nomen\"][id][\"ARTIKEL\"]== \"die\":\n",
    "        nomen_w.append(result_nomen[\"Nomen\"][id][\"WORT\"])\n",
    "    elif result_nomen [\"Nomen\"][id][\"ARTIKEL\"]== \"das\":\n",
    "        nomen_n.append(result_nomen[\"Nomen\"][id][\"WORT\"])\n",
    "    else:\n",
    "        pass\n",
    "with open('Nomen_m.json', 'w') as fp:\n",
    "      dict_result_nomen=json.dump(nomen_m, fp, indent=4)\n",
    "with open('Nomen_w.json', 'w') as fp:\n",
    "      dict_result_nomen=json.dump(nomen_w, fp, indent=4)\n",
    "with open('Nomen_n.json', 'w') as fp:\n",
    "      dict_result_nomen=json.dump(nomen_n, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRONOMEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronomen nach männlich und weiblich unterteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['er', 'der', 'Er', 'er', 'Er', 'er', 'er', 'ihn', 'ihm', 'der', 'er', 'er', 'der', 'er', 'er', 'der', 'der', 'er', 'der', 'der', 'ihn', 'ihn', 'ihn', 'der', 'ihn', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'ihn', 'ihn', 'ihn', 'ihm', 'Er', 'Er', 'er', 'er', 'er', 'er', 'er', 'er', 'er', 'er', 'er', 'der', 'der', 'ihn', 'ihn', 'ihn', 'ihn', 'er', 'er', 'er', 'er', 'dieser', 'dieser', 'dieser', 'ihn', 'er', 'er', 'ihn', 'ihm', 'ihm', 'der', 'der', 'der', 'der', 'der']\n",
      "['Sie', 'Sie', 'Sie', 'sie', 'sie', 'Sie', 'sie', 'sie', 'die', 'sie', 'sie', 'Die', 'die', 'sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'Sie', 'Sie', 'sie', 'sie', 'Sie', 'sie', 'sie', 'sie', 'die', 'sie', 'die', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'sie', 'sie', 'sie', 'ihr', 'die', 'die', 'die', 'sie', 'sie', 'sie', 'die', 'die', 'sie', 'Sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'Sie', 'Sie', 'Sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'sie', 'Sie', 'Die', 'Die', 'sie', 'sie', 'sie', 'die', 'die', 'die', 'die', 'sie', 'sie', 'DIESE', 'die', 'die', 'die', 'ihr', 'die', 'ihr', 'die', 'ihr', 'die', 'sie', 'die', 'sie', 'sie', 'sie', 'sie', 'sie', 'die', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'ihr', 'ihr', 'sie', 'sie', 'Sie', 'sie', 'ihr', 'Sie', 'sie', 'Sie', 'sie', 'ihr', 'die', 'die', 'die', 'DIE', 'die', 'sie', 'Sie', 'sie', 'sie', 'sie', 'Sie', 'Sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'sie', 'Sie', 'Sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'die', 'sie', 'sie', 'Sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'sie', 'sie', 'sie', 'Sie', 'sie', 'sie', 'Sie', 'sie', 'sie', 'sie', 'sie', 'die', 'sie', 'sie', 'sie', 'Sie', 'sie', 'sie', 'sie', 'ihr', 'sie', 'die', 'Sie', 'ihr', 'ihr', 'diese', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'Sie', 'Sie', 'Sie', 'Sie', 'Sie', 'sie', 'sie', 'sie', 'Ihr', 'sie', 'sie', 'sie', 'Die', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'die', 'ihr', 'sie', 'die', 'sie', 'sie', 'sie', 'ihr', 'sie', 'sie', 'ihr', 'ihr', 'die', 'die', 'sie', 'die', 'sie', 'die', 'sie', 'sie', 'sie', 'die', 'die', 'sie', 'sie', 'sie', 'sie', 'sie', 'Sie', 'sie', 'Sie', 'die', 'sie', 'Sie', 'sie', 'sie', 'Sie', 'sie', 'die', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'die', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'die', 'Sie', 'Sie', 'Sie', 'sie', 'ihr', 'Sie', 'Sie', 'sie', 'sie', 'sie', 'Sie', 'Sie', 'Sie', 'Sie', 'Sie', 'sie', 'ihr', 'Sie', 'sie', 'Sie', 'sie', 'sie', 'Sie', 'Sie', 'Sie', 'Sie', 'Sie', 'sie', 'ihr', 'Sie', 'Sie', 'sie', 'sie', 'sie', 'sie', 'sie', 'ihr', 'sie', 'Sie', 'ihr', 'sie', 'sie', 'Sie', 'Sie', 'Sie', 'Sie', 'sie', 'Sie', 'Sie', 'Sie', 'sie', 'Sie', 'Sie', 'Sie', 'sie', 'Sie', 'Sie', 'Sie', 'ihr', 'ihr', 'Sie', 'sie', 'sie', 'sie', 'Sie', 'Sie', 'Sie', 'sie', 'Sie', 'Sie', 'Sie', 'sie', 'sie', 'Sie', 'die', 'sie', 'Sie', 'Sie', 'sie', 'die', 'DIESE', 'sie', 'sie', 'sie']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pronomen_m, pronomen_w = [],[]\n",
    "\n",
    "for word in list_of_pronouns:\n",
    "    #männlich\n",
    "    #personal\n",
    "    if word == \"er\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Er\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"ER\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"ihn\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Ihn\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"IHN\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"ihm\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Ihm\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"IHM\":\n",
    "        pronomen_m.append(word)\n",
    "    #possesiv\n",
    "    elif word == \"sein\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Sein\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"SEIN\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"seine\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Seine\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"SEINE\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Seiner\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"seiner\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"SEINER\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Seines\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"seines\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"SEINES\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Seinen\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"seinen\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"SEINEN\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Seinem\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"seinem\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"SEINEM\":\n",
    "        pronomen_m.append(word)\n",
    "    #demonstrativ\n",
    "    elif word == \"Der\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"der\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"DER\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Dieser\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"DIESER\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"dieser\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Derjenige\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"DERJENIGE\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"derjenige\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Derselbe\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"DERSELBE\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"derselbe\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"Jener\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"jener\":\n",
    "        pronomen_m.append(word)\n",
    "    elif word == \"JENER\":\n",
    "        pronomen_m.append(word)\n",
    "    #weiblich\n",
    "    #personal\n",
    "    if word == \"sie\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Sie\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"SIE\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"ihr\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"IHR\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Ihr\":\n",
    "        pronomen_w.append(word)\n",
    "    #possessiv\n",
    "    elif word == \"Ihre\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"IHRE\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"ihre\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"IHREN\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Ihren\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"ihren\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"IHREM\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"ihrem\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Ihrem\":\n",
    "        pronomen_w.append(word)\n",
    "    #demonstrativ\n",
    "    elif word == \"Die\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"DIE\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"die\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Diejenige\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"diejenige\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"DIEJENIGE\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"DIESELBE\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"dieselbe\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Dieselbe\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"DIESE\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"diese\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Diese\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"Jene\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"JENE\":\n",
    "        pronomen_w.append(word)\n",
    "    elif word == \"jene\":\n",
    "        pronomen_w.append(word)\n",
    "   \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "with open('Pronomen_m.json', 'w') as fp:\n",
    "      dict_result_nomen=json.dump(pronomen_m, fp, indent=4)\n",
    "with open('Pronomen_w.json', 'w') as fp:\n",
    "      dict_result_nomen=json.dump(pronomen_w, fp, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
